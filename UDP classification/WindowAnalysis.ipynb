{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifiers(data, seconds, save_dir):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from sklearn import svm\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier  \n",
    "    \n",
    "    \n",
    "    dataset = pd.read_csv(\"/home/det_tesi/sgarofalo/Window size analysis/train_dataset/train_dataset.csv\", header = [0, 1])\n",
    "    colors = ['r', 'b']\n",
    "    classes = [0, 1] # 0 = Not RTP and 1 = RTP\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1:]\n",
    "    classes_str, y = np.unique(y, return_inverse=True)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    columns = [\"interarrival_std\", \"interarrival_mean\", \"len_udp_std\", \"len_udp_mean\", \n",
    "               \"udp_count\", \"len_udp_kbps\" , \"interlength_udp_mean\"]\n",
    "    print(\"Classifing with \" + str(seconds) +\" window size..\")\n",
    "    print(\"Training set classes distribution: %.2f%% RTP\" % (100*len(y[y == 1])/len(y)))\n",
    "    # #############################################################################\n",
    "    # Correlation Matrix\n",
    "    # #############################################################################\n",
    "    corr = pd.DataFrame(data=X, columns = columns).corr()\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    plt.figure(figsize = (10,10))\n",
    "    ax = sns.heatmap(corr, \n",
    "                xticklabels=corr.columns.values,\n",
    "                yticklabels=corr.columns.values,\n",
    "                annot = True, cmap=\"YlGnBu\", mask = mask)\n",
    "    t = \"Correlation matrix\"\n",
    "    save_photo(save_dir, t, str(seconds)+\"s\")\n",
    "    \n",
    "    # #############################################################################\n",
    "    # Plot Data Using PCA\n",
    "    # #############################################################################\n",
    "    myModel = PCA(2)\n",
    "    PC = myModel.fit_transform(X)\n",
    "    # print (\"%.2f%% variance ratio with 2 PC\" % (100*sum(myModel.explained_variance_ratio_)))\n",
    "    principalDf = pd.DataFrame(data = np.column_stack((PC[:, 0:2], y)), columns = ['principal component 1', 'principal component 2', 'label'])\n",
    "    fig = plt.figure(figsize = (13,13))\n",
    "    #plt.scatter(principalDf.iloc[:, 0], principalDf.iloc[:, 1])\n",
    "    for target, color in zip(classes, colors):\n",
    "        indicesToKeep = principalDf['label'] == target\n",
    "        plt.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
    "                   , principalDf.loc[indicesToKeep, 'principal component 2']\n",
    "                   , c = color, s = 40)\n",
    "    plt.xlabel('Principal Component 1', fontsize = 15)\n",
    "    plt.ylabel('Principal Component 2', fontsize = 15, labelpad = -10)\n",
    "    plt.legend(classes_str)\n",
    "    plt.grid()\n",
    "    t = \"Data Plotting using PCA\"\n",
    "    plt.title(t, fontsize = 10)\n",
    "    save_photo(save_dir, t, str(seconds)+\"s\")\n",
    "    \n",
    "    # #############################################################################\n",
    "    # SVM\n",
    "    # #############################################################################\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    model = svm.SVC()\n",
    "    C = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "    kernel = ['rbf']\n",
    "    gamma = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    params = {'C': C, 'kernel': kernel, 'gamma': gamma}\n",
    "    SVM = GridSearchCV(model, params, cv=5, n_jobs=-1, iid=True)\n",
    "    SVM.fit(X_train, y_train)\n",
    "    SVMAccuracy = SVM.best_estimator_.score(X_test, y_test)\n",
    "    \n",
    "    # #############################################################################\n",
    "    # Random Forest\n",
    "    # #############################################################################\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "    model = RandomForestClassifier()\n",
    "    max_features = [3, 5, 7]\n",
    "    n_estimators = [100, 200, 500, 1000, 2000]\n",
    "    param_grid = {\n",
    "        'max_features': max_features,\n",
    "        'n_estimators': n_estimators\n",
    "    }\n",
    "    RF = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "    RF.fit(X_train, y_train)\n",
    "    \n",
    "    RFAccuracy = RF.best_estimator_.score(X_test, y_test)\n",
    "    feature_imp = pd.Series(RF.best_estimator_.feature_importances_, index=columns)\n",
    "    fig = plt.figure(figsize = (13,13))\n",
    "    sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "    # Add labels to your graph\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.legend()\n",
    "    t = \"Important Features\"\n",
    "    plt.title(t, fontsize = 10)\n",
    "    save_photo(save_dir, t, str(seconds)+\"s\")\n",
    "    \n",
    "    # #############################################################################\n",
    "    # KNN\n",
    "    # #############################################################################\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "    model = KNeighborsClassifier()\n",
    "    metric = [\"manhattan\", \"euclidean\", \"chebyshev\"]\n",
    "    weights = ['uniform', 'distance']\n",
    "    params = {\"metric\": metric, 'weights': weights, 'n_neighbors': range(1, 40)}\n",
    "    KNN = GridSearchCV(model, params, cv=5, n_jobs=-1)\n",
    "    KNN.fit(X_train, y_train)\n",
    "    KNNAccuracy = KNN.best_estimator_.score(X_test, y_test)\n",
    "\n",
    "    # #############################################################################\n",
    "    # Test Set\n",
    "    # #############################################################################  \n",
    "    dataset = pd.read_csv(\"/home/det_tesi/sgarofalo/Window size analysis/test_dataset/test_dataset.csv\", header = [0, 1])\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1:]\n",
    "    classes_str, y = np.unique(y, return_inverse=True)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    print()\n",
    "    print(\"test_dataset shape: \" + str(dataset.shape))\n",
    "    SVMAccuracy = SVM.best_estimator_.score(X, y)\n",
    "    print(\"SVM accuracy => %.2f%%\" % (100*SVMAccuracy))\n",
    "    RFAccuracy = RF.best_estimator_.score(X, y)\n",
    "    print(\"RF accuracy => %.2f%%\" % (100*RFAccuracy))\n",
    "    KNNAccuracy = KNN.best_estimator_.score(X, y)\n",
    "    print(\"KNN accuracy => %.2f%%\" % (100*KNNAccuracy))\n",
    "    \n",
    "    hist_data = {\"SVM\": [SVMAccuracy], \n",
    "                 \"Random Forest\": [RFAccuracy],\n",
    "                 \"KNN\": [KNNAccuracy]}\n",
    "    sns.barplot(data = pd.DataFrame(data=hist_data))\n",
    "    t = \"Classifiers Comparison\"\n",
    "    plt.title(t, fontsize = 10)\n",
    "    save_photo(save_dir, t, str(seconds)+\"s\")\n",
    "    \n",
    "    data[seconds] = {}\n",
    "    data[seconds][\"SVM\"] = SVMAccuracy \n",
    "    data[seconds][\"RF\"] = RFAccuracy\n",
    "    data[seconds][\"KNN\"] = KNNAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from json import JSONDecoder, JSONDecodeError\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def decode_stacked(document, pos=0, decoder=JSONDecoder()):\n",
    "    NOT_WHITESPACE = re.compile(r'[^\\s]')\n",
    "    while True:\n",
    "        match = NOT_WHITESPACE.search(document, pos)\n",
    "        if not match:\n",
    "            return\n",
    "        pos = match.start()\n",
    "\n",
    "        try:\n",
    "            obj, pos = decoder.raw_decode(document, pos)\n",
    "        except JSONDecodeError:\n",
    "            # do something sensible if there's some error\n",
    "            raise\n",
    "        yield obj\n",
    "        \n",
    "\n",
    "def save_photo(save_dir, t, window_size=None):\n",
    "    if sys.platform.startswith(\"linux\"):\n",
    "        save_dir = save_dir+'/Plots/'\n",
    "        save_dir_flow = save_dir + window_size + '/'\n",
    "    elif sys.platform.startswith(\"win32\"):\n",
    "        save_dir = save_dir+r'\\\\Plots\\\\'\n",
    "        save_dir_flow = save_dir + window_size + r'\\\\'    \n",
    "    dpi = 100\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    if window_size == None:\n",
    "        plt.savefig(save_dir + t +'.png', dpi = dpi)\n",
    "    else:\n",
    "        if not os.path.exists(save_dir_flow):\n",
    "            os.makedirs(save_dir_flow)\n",
    "        plt.savefig(save_dir_flow + t +'.png', dpi = dpi)      \n",
    "    plt.close()\n",
    "    \n",
    "#Open Tshark and run command to turn pcap to json\n",
    "def pcap_to_json(source_pcap, protocol_policy):\n",
    "    import subprocess\n",
    "        \n",
    "    # Retrive all STUN packets\n",
    "    command = ['tshark', '-r', source_pcap, '-l', '-n', '-T', 'ek', '-Y (stun)']\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, encoding = 'utf-8', )\n",
    "    try:\n",
    "        output, error = process.communicate()\n",
    "    except Exception as e:\n",
    "        print (\"Errore in pcap_to_json \" + str(e))\n",
    "        process.kill()\n",
    "\n",
    "    # I've got all STUN packets: need to find which ports are used by RTP\n",
    "    used_port = set()\n",
    "    for obj in decode_stacked(output):\n",
    "        if 'index' in obj.keys():\n",
    "            continue\n",
    "        if 'stun' in obj['layers'].keys() and \"0x00000101\" in obj['layers'][\"stun\"][\"stun_stun_type\"]:          #0x0101 means success\n",
    "            used_port.add(obj['layers'][\"udp\"][\"udp_udp_srcport\"])\n",
    "            used_port.add(obj['layers'][\"udp\"][\"udp_udp_dstport\"])\n",
    "    \n",
    "    command = ['tshark', '-r', source_pcap, '-l', '-n', '-T', 'ek', protocol_policy, 'rtp']\n",
    "    for port in used_port:\n",
    "        command.append(\"-d udp.port==\" + port + \",rtp\")\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, encoding = 'utf-8', stderr=None)\n",
    "    try:\n",
    "        output, error = process.communicate()\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print (\"Errore in pcap_to_json \" + str(e))\n",
    "        process.kill()\n",
    "    return output\n",
    "\n",
    "\n",
    "#Read json created with tshark and put it in a list\n",
    "def json_to_list(output):\n",
    "    \n",
    "    def udp_insert(obj, unique_flow, dict_flow_data, dictionary):\n",
    "        \n",
    "        # Retrive flow information \n",
    "        if \"ipv6\" in obj['layers']:\n",
    "            source_addr = obj['layers']['ipv6']['ipv6_ipv6_src']\n",
    "            dest_addr = obj['layers']['ipv6']['ipv6_ipv6_dst']\n",
    "        elif \"ip\" in obj[\"layers\"]:\n",
    "            source_addr = obj['layers']['ip']['ip_ip_src']\n",
    "            dest_addr = obj['layers']['ip']['ip_ip_dst']\n",
    "        source_port = int(obj['layers']['udp']['udp_udp_srcport'])\n",
    "        dest_port = int(obj['layers']['udp']['udp_udp_dstport'])\n",
    "        \n",
    "        # Save ssrc if new\n",
    "        unique_tuple = (source_addr, dest_addr, source_port, dest_port)\n",
    "        unique_flow.add(unique_tuple)\n",
    "\n",
    "        # Retrive packet information\n",
    "        frame_num = int(obj['layers']['frame']['frame_frame_number'])       \n",
    "        len_udp = int(obj['layers']['udp']['udp_udp_length'])\n",
    "        len_frame = int(obj['layers']['frame']['frame_frame_len'])\n",
    "        timestamp = float(obj['layers']['frame']['frame_frame_time_epoch'])\n",
    "        label = \"RTP\" if 'rtp' in obj['layers'].keys() else \"Not RTP\"\n",
    "        # Add new packet to dictionary\n",
    "#        columns = ['frame_num', 'p_type', 'len_udp', 'len_ip', 'len_frame', 'timestamps', 'rtp_timestamp', 'rtp_seq_num']\n",
    "        data = [frame_num, len_udp, len_frame, timestamp, label]\n",
    "        \n",
    "        if unique_tuple in dictionary:\n",
    "            dictionary[unique_tuple].append(data)\n",
    "        else:\n",
    "            dictionary[unique_tuple] = []\n",
    "            dictionary[unique_tuple].append(data)\n",
    "    \n",
    "    dict_data = {}\n",
    "\n",
    "    #Find RTP flows\n",
    "    unique_flow = set()\n",
    "    dict_flow_data = {}\n",
    "    \n",
    "    # df containign unique flow\n",
    "    df_unique_flow = pd.DataFrame(columns = ['source_addr',\n",
    "                           'dest_addr',\n",
    "                           'source_port',\n",
    "                           'dest_port'])\n",
    "    \n",
    "\n",
    "    # Analyze each packet\n",
    "    for obj in decode_stacked(output):\n",
    "        #remove instances which have only index:date and type:pcap_file\n",
    "        if 'index' in obj.keys():\n",
    "            continue\n",
    "        elif 'udp' in obj['layers'].keys():\n",
    "            udp_insert(obj, unique_flow, dict_flow_data, dict_data)\n",
    "\n",
    "    for x in unique_flow:\n",
    "        columns = ['frame_num', 'len_udp', 'len_frame', 'timestamps', 'label']\n",
    "        dict_flow_data[x] = pd.DataFrame(dict_data[x], columns=columns)\n",
    "        df_unique_flow = df_unique_flow.append({'source_addr': x[0], 'dest_addr': x[1], \n",
    "                'source_port': x[2], 'dest_port': x[3]}, ignore_index = True)\n",
    "        \n",
    "    return dict_flow_data, df_unique_flow, unique_flow\n",
    "\n",
    "#    return df_rtp, l_rtp, l_non_rtp, l_stun, l_rtcp, l_turn, l_tcp, l_only_udp, unique_flow\n",
    "\n",
    "\n",
    "#Create nicknames for every flow\n",
    "def make_nicknames(dict_flow_data):\n",
    "    \n",
    "    #New: packet lenght mean -> audio/video\n",
    "    dict_flow_nickname = {}\n",
    "    threshold = 300\n",
    "    for key in dict_flow_data:\n",
    "        mean = dict_flow_data[key][\"len_frame\"].mean()\n",
    "        if mean > threshold:\n",
    "            dict_flow_nickname[key] = \"video\"\n",
    "        else:\n",
    "            dict_flow_nickname[key] = \"audio\"\n",
    "#    \n",
    "    \n",
    "\n",
    "\n",
    "    return dict_flow_nickname\n",
    "\n",
    "\n",
    "\n",
    "#Make dictionary flow : df, bytes_per_second, packets_per_second\n",
    "def make_rtp_data(dict_flow_data):\n",
    "        \n",
    "    \n",
    "    packets_per_second = {}\n",
    "    bytes_per_second = {}\n",
    "    inter_packet_gap_s = {}\n",
    "    inter_rtp_timestamp_gap = {}\n",
    "    \n",
    "    for flow_id in dict_flow_data:\n",
    "        inner_df = dict_flow_data[flow_id].sort_values('timestamps')\n",
    "        inter_packet_gap_s[flow_id] = (inner_df.timestamps - inner_df.timestamps.shift()).dropna()\n",
    "        inter_rtp_timestamp_gap[flow_id] = (inner_df.rtp_timestamp - inner_df.rtp_timestamp.shift()).dropna()\n",
    "        \n",
    "        # Need to define an index in order to use resample method\n",
    "        datetime = pd.to_datetime(inner_df.timestamps, unit = 's')\n",
    "        inner_df = inner_df.set_index(datetime)\n",
    "        packets_per_second[flow_id] = inner_df.iloc[:,0].resample('S').count()\n",
    "        bytes_per_second[flow_id] = inner_df['len_frame'].resample('S').sum()\n",
    "        \n",
    "    return packets_per_second, bytes_per_second, inter_packet_gap_s, inter_rtp_timestamp_gap\n",
    "\n",
    "\n",
    "\n",
    "def calculate_packet_loss(dict_flow_data):\n",
    "     \n",
    "    #Calculate packet loss\n",
    "    dict_flow_packet_loss = {}\n",
    "    for flow_id in dict_flow_data:\n",
    "        seq = dict_flow_data[flow_id]['rtp_seq_num'].sort_values()\n",
    "        seq_diff = (seq - seq.shift()).fillna(1)\n",
    "        dict_flow_packet_loss[flow_id] = (seq_diff.where(seq_diff != 1)-1).sum()\n",
    "    \n",
    "#    print(\"Packet losses: \", dict_flow_packet_loss)\n",
    "    \n",
    "    return dict_flow_packet_loss\n",
    "#    \n",
    "\n",
    "\n",
    "def inter_statistics(dict_flow_data):\n",
    "    for flow_id in dict_flow_data:\n",
    "        dict_flow_data[flow_id][\"interarrival\"] = dict_flow_data[flow_id][\"timestamps\"].diff()\n",
    "        dict_flow_data[flow_id][\"interlength_udp\"] = dict_flow_data[flow_id][\"len_udp\"].diff()\n",
    "#        dict_flow_data[flow_id][\"label\"] = 0 if dict_flow_data[flow_id][\"len_udp\"].max() > 500 else 1\n",
    "    return dict_flow_data\n",
    "\n",
    "def kbps(series):\n",
    "    return series.sum()*8/1024\n",
    "\n",
    "def max_count(series):\n",
    "    a = len(series[series == \"RTP\"]) > len(series[series == \"Not RTP\"])\n",
    "    return \"RTP\" if a else \"Not RTP\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building datasets with seconds_samples = 1s\n",
      "/home/det_tesi/sgarofalo/Window size analysis/train_dataset/python_tm.pcap\n",
      "/home/det_tesi/sgarofalo/Window size analysis/train_dataset/python_tm_2.pcap\n",
      "/home/det_tesi/sgarofalo/Window size analysis/train_dataset/Simone_cattura_webex_app_ethernet_07_11_2_people_video.pcapng\n",
      "/home/det_tesi/sgarofalo/Window size analysis/train_dataset/Simone_cattura_webex_app_ethernet_07_11_2_people_audio_video_doing_nothing.pcapng\n",
      "/home/det_tesi/sgarofalo/Window size analysis/train_dataset/udp_nonRTP_traffic.pcapng\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for seconds in range(1, 11):\n",
    "    seconds_samples = str(seconds) + \"s\"\n",
    "    save_dir = \"/home/det_tesi/sgarofalo/Window size analysis\"\n",
    "    print(\"Building datasets with seconds_samples = \" + seconds_samples)\n",
    "    for source_pcap, filename in zip([\"/home/det_tesi/sgarofalo/Window size analysis/train_dataset\", \"/home/det_tesi/sgarofalo/Window size analysis/test_dataset\"], [\"train_dataset.csv\", \"test_dataset.csv\"]):        \n",
    "        arr = os.listdir(source_pcap)\n",
    "        arr = [x for x in arr if x.split(\".\")[1].startswith((\"pcapng\", \"pcap\"))]\n",
    "        df_merged = pd.DataFrame()\n",
    "        for file in arr:\n",
    "            file_path = source_pcap + \"/\" + file\n",
    "            protocol_policy = \"--disable-protocol\" if 'game' in file else '--enable-protocol'\n",
    "            output = pcap_to_json(file_path, protocol_policy)\n",
    "            dict_flow_data, df_unique_flow, unique_flow = json_to_list(output)\n",
    "            dict_flow_data = inter_statistics(dict_flow_data) \n",
    "            df_train = pd.DataFrame()\n",
    "            for flow_id in dict_flow_data.keys():\n",
    "                dict_flow_data[flow_id][\"timestamps\"] = pd.to_datetime(dict_flow_data[flow_id][\"timestamps\"], unit = 's')\n",
    "                dict_flow_data[flow_id].set_index('timestamps', inplace = True)\n",
    "                train = dict_flow_data[flow_id].resample(seconds_samples).agg({'interarrival' : ['std', 'mean'], 'len_udp' : ['std', 'mean', 'count', kbps], \\\n",
    "                            'interlength_udp' : ['mean'], \"label\": max_count})\n",
    "                df_train = pd.concat([df_train, train])\n",
    "            df_train_dropped = df_train.dropna()\n",
    "            #print(\"[%s]: dataset shape: %s\" % (file.split(\".\")[0], str(df_train_dropped.shape)))\n",
    "            y = df_train_dropped.label.max_count\n",
    "            #print(\"[%s]: dataset classes distribution: %.2f%% RTP\" % (file.split(\".\")[0], 100*len(y[y == \"RTP\"])/len(y)))\n",
    "            df_merged = pd.concat([df_merged, df_train_dropped], ignore_index=True)\n",
    "        df_merged.to_csv(source_pcap + \"/\" + filename, sep=',', encoding='utf-8', index=False)\n",
    "    classifiers(data, seconds, save_dir)\n",
    "    print()\n",
    "columns = [\"SVM\", \"RF\", \"KNN\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "for i in data:\n",
    "    df = df.append({\"SVM\": data[i][\"SVM\"], \"RF\": data[i][\"RF\"], \"KNN\": data[i][\"KNN\"]}, ignore_index=True)\n",
    "df.index +=  1 \n",
    "plt.figure(figsize=(20, 16))\n",
    "plt.plot(df)\n",
    "plt.xlabel(\"Seconds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(columns, fontsize = 16)\n",
    "t = \"Window size analysis\"\n",
    "plt.title(t, fontsize=16)\n",
    "path = \"/home/det_tesi/sgarofalo/Window size analysis/result\"\n",
    "plt.savefig(path + \"/\" + t + '.png', dpi = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
